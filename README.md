# Explain-This-AI

Explain-This-AI is an AI-powered learning assistant that extracts text from images using OCR, retrieves the most relevant context using vector embeddings (RAG), and generates clear explanations using a large language model.

The goal is to help students better understand AI/ML slides, diagrams, and notes by turning visual content into grounded, structured explanations.

---

## ğŸš€ Features

- ğŸ“· Upload slide or diagram images
- ğŸ” OCR-based text extraction
- ğŸ§  Embedding-based semantic retrieval (RAG pipeline)
- ğŸ¤– LLM-powered explanation generation
- ğŸ“š Source-aware answers (top-k relevant chunks)
- ğŸ’» Full-stack architecture (FastAPI + React)

---

## ğŸ—ï¸ System Architecture

```
User Upload
     â†“
OCR (extract text)
     â†“
Text Cleaning
     â†“
Chunking
     â†“
Embeddings (SentenceTransformers)
     â†“
Cosine Similarity Search
     â†“
Top-K Relevant Chunks
     â†“
LLM (OpenAI)
     â†“
Structured Explanation + Sources
```

---

## ğŸ› ï¸ Tech Stack

### Backend
- FastAPI
- Python
- SentenceTransformers
- NumPy
- EasyOCR
- OpenAI API
- Cosine Similarity Retrieval

### Frontend
- React
- Axios

---

## ğŸ“‚ Project Structure

```
explain-this-ai/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ uploaded_files/ (ignored)
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ public/
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## âš™ï¸ Local Setup

### 1ï¸âƒ£ Clone the repository

```
git clone https://github.com/YOUR_USERNAME/explain-this-ai.git
cd explain-this-ai
```

---

### 2ï¸âƒ£ Backend Setup

```
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

Create a `.env` file inside `backend/`:

```
OPENAI_API_KEY=your_key_here
```

Run backend:

```
uvicorn app.main:app --reload
```

Backend runs at:

```
http://127.0.0.1:8000
```

---

### 3ï¸âƒ£ Frontend Setup

```
cd frontend
npm install
npm start
```

Frontend runs at:

```
http://localhost:3000
```

---

## ğŸ§  How Retrieval (RAG) Works

1. OCR extracts raw text from the uploaded image.
2. Text is cleaned and split into chunks.
3. Each chunk is embedded using SentenceTransformers.
4. The userâ€™s question is embedded.
5. Cosine similarity finds the top-k most relevant chunks.
6. Those chunks are passed to the LLM.
7. The LLM generates a grounded explanation using retrieved context.

This ensures responses are based on extracted slide content rather than hallucinated information.

---

## ğŸ” Security Notes

- `.env` files are excluded from version control.
- API keys should never be committed.
- Uploaded files and cache directories are ignored via `.gitignore`.

---

## ğŸ“ˆ Future Improvements

- Persistent vector database (FAISS / pgvector)
- Streaming LLM responses
- Multi-document support
- Improved UI/UX
- Dockerized deployment
- Cloud deployment (AWS / Render)

---

## ğŸ‘¨â€ğŸ’» Author

Arjun Bharadwaj  
Computer Science @ University of Maryland  
AWS Certified Cloud Practitioner  

---

## ğŸ“„ License

MIT License
